{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc96706e",
   "metadata": {},
   "source": [
    "# Comprehensive Generative AI Application\n",
    "## Text, Image, and Code Generation using Free/Open-Source Models\n",
    "\n",
    "This notebook demonstrates multiple generative AI capabilities using Hugging Face models and Ollama for local LLM execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e02985",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Install and import all necessary dependencies for generative AI tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    'transformers',\n",
    "    'torch',\n",
    "    'diffusers',\n",
    "    'pillow',\n",
    "    'requests',\n",
    "    'accelerate',\n",
    "    'safetensors'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úì {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21616f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU (slower, but will work)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4584e2f",
   "metadata": {},
   "source": [
    "## 2. Set Up Free/Open-Source Models\n",
    "\n",
    "Configure models from Hugging Face and set up device management for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732850f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model configurations for different tasks\n",
    "MODEL_CONFIGS = {\n",
    "    \"text_generation\": {\n",
    "        \"gpt2\": \"gpt2\",\n",
    "        \"distilgpt2\": \"distilgpt2\",  # Smaller, faster version\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"t5-small\": \"t5-small\",\n",
    "        \"distilbart\": \"sshleifer/distilbart-cnn-6-6\"\n",
    "    },\n",
    "    \"question_answering\": {\n",
    "        \"distilbert\": \"distilbert-base-cased-distilled-squad\"\n",
    "    },\n",
    "    \"image_generation\": {\n",
    "        \"stable_diffusion\": \"runwayml/stable-diffusion-v1-5\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store loaded models\n",
    "loaded_models = {}\n",
    "\n",
    "def load_model_with_status(model_name, task_type, use_fp16=False):\n",
    "    \"\"\"Load a model and report status\"\"\"\n",
    "    print(f\"\\nüì• Loading {task_type} model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if task_type == \"text_generation\":\n",
    "            pipe = pipeline(\"text-generation\", model=model_name, device=0 if device == \"cuda\" else -1)\n",
    "        elif task_type == \"summarization\":\n",
    "            pipe = pipeline(\"summarization\", model=model_name, device=0 if device == \"cuda\" else -1)\n",
    "        elif task_type == \"question_answering\":\n",
    "            pipe = pipeline(\"question-answering\", model=model_name, device=0 if device == \"cuda\" else -1)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì Successfully loaded in {elapsed:.2f} seconds\")\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load GPT-2 for text generation\n",
    "gpt2_model = load_model_with_status(\"distilgpt2\", \"text_generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126de938",
   "metadata": {},
   "source": [
    "## 3. Text Generation with Hugging Face\n",
    "\n",
    "Generate creative text using pre-trained transformer models with various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe385d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, max_length=100, temperature=0.7, top_p=0.9, num_outputs=1):\n",
    "    \"\"\"\n",
    "    Generate text using a language model\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt: Starting text\n",
    "    - model: Loaded pipeline model\n",
    "    - max_length: Maximum length of generated text\n",
    "    - temperature: Controls randomness (0=deterministic, 1=more random)\n",
    "    - top_p: Nucleus sampling parameter\n",
    "    - num_outputs: Number of outputs to generate\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ Generating text from prompt: '{prompt}'\")\n",
    "    print(f\"Parameters: max_length={max_length}, temperature={temperature}, top_p={top_p}\")\n",
    "    \n",
    "    try:\n",
    "        outputs = model(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            num_return_sequences=num_outputs,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        for i, output in enumerate(outputs):\n",
    "            print(f\"\\n--- Generation {i+1} ---\")\n",
    "            print(output['generated_text'])\n",
    "        \n",
    "        return outputs\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example 1: Simple creative writing\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE 1: Creative Story Generation\")\n",
    "print(\"=\" * 60)\n",
    "generate_text(\n",
    "    \"Once upon a time in a magical forest\",\n",
    "    gpt2_model,\n",
    "    max_length=80,\n",
    "    temperature=0.9,\n",
    "    num_outputs=1\n",
    ")\n",
    "\n",
    "# Example 2: Technical documentation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE 2: Technical Content Generation\")\n",
    "print(\"=\" * 60)\n",
    "generate_text(\n",
    "    \"Python decorators are useful for\",\n",
    "    gpt2_model,\n",
    "    max_length=100,\n",
    "    temperature=0.5,\n",
    "    num_outputs=1\n",
    ")\n",
    "\n",
    "# Example 3: Multiple variations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE 3: Multiple Variations\")\n",
    "print(\"=\" * 60)\n",
    "generate_text(\n",
    "    \"The future of artificial intelligence\",\n",
    "    gpt2_model,\n",
    "    max_length=80,\n",
    "    temperature=0.8,\n",
    "    num_outputs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093dbf12",
   "metadata": {},
   "source": [
    "## 4. Image Generation with Stable Diffusion\n",
    "\n",
    "Generate images from text prompts using the Stable Diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Stable Diffusion model...\")\n",
    "print(\"(This may take a while on first run)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Load Stable Diffusion pipeline\n",
    "    print(\"üì• Loading Stable Diffusion...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "    )\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úì Successfully loaded in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    stable_diffusion_loaded = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Note: Stable Diffusion requires significant resources\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"We'll skip image generation for now\")\n",
    "    stable_diffusion_loaded = False\n",
    "\n",
    "def generate_image(prompt, num_steps=50, guidance_scale=7.5):\n",
    "    \"\"\"\n",
    "    Generate images from text prompts\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt: Text description of image\n",
    "    - num_steps: Number of inference steps (more = better but slower)\n",
    "    - guidance_scale: Controls adherence to prompt (higher = more adherent)\n",
    "    \"\"\"\n",
    "    if not stable_diffusion_loaded:\n",
    "        print(\"‚ö† Stable Diffusion is not loaded. Skipping image generation.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüé® Generating image from prompt: '{prompt}'\")\n",
    "    print(f\"Parameters: num_steps={num_steps}, guidance_scale={guidance_scale}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=num_steps,\n",
    "            guidance_scale=guidance_scale\n",
    "        ).images[0]\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì Image generated in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example image generation (optional - only if model loaded successfully)\n",
    "if stable_diffusion_loaded:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE: Image Generation\")\n",
    "    print(\"=\" * 60)\n",
    "    image = generate_image(\n",
    "        \"A serene mountain landscape with a lake at sunset, oil painting style\",\n",
    "        num_steps=30,\n",
    "        guidance_scale=7.5\n",
    "    )\n",
    "    \n",
    "    if image:\n",
    "        image.save(\"generated_image.png\")\n",
    "        print(\"‚úì Image saved as 'generated_image.png'\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Image Generation - Resource Limitation\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üí° Stable Diffusion requires 6-8GB VRAM for optimal performance\")\n",
    "    print(\"üí° For image generation, consider using an online API or local Ollama setup\")\n",
    "    print(\"üí° You can modify the model to a lighter version if available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dcd604",
   "metadata": {},
   "source": [
    "## 5. Code Generation with Open-Source Models\n",
    "\n",
    "Generate Python code snippets from natural language descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CodeGen or similar model for code generation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Code Generation Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    code_gen_model = load_model_with_status(\"Salesforce/codegen-350M-mono\", \"text_generation\")\n",
    "    code_gen_loaded = True\n",
    "except:\n",
    "    code_gen_loaded = False\n",
    "    print(\"‚ö† CodeGen model not available, using GPT-2 for code examples\")\n",
    "    code_gen_model = gpt2_model\n",
    "\n",
    "def generate_code(description, model, max_length=150, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Generate Python code from a natural language description\n",
    "    \n",
    "    Parameters:\n",
    "    - description: What the code should do\n",
    "    - model: Loaded code generation model\n",
    "    - max_length: Max tokens to generate\n",
    "    - temperature: Creativity level\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíª Generating code for: '{description}'\")\n",
    "    \n",
    "    # Create a prompt in the format the model understands\n",
    "    prompt = f\"# {description}\\n\"\n",
    "    \n",
    "    try:\n",
    "        outputs = model(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        generated_code = outputs[0]['generated_text']\n",
    "        print(\"\\n--- Generated Code ---\")\n",
    "        print(generated_code)\n",
    "        \n",
    "        return generated_code\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Code generation examples\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE 1: Function Generation\")\n",
    "print(\"=\" * 60)\n",
    "generate_code(\n",
    "    \"Function to calculate factorial of a number\",\n",
    "    code_gen_model,\n",
    "    max_length=120,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE 2: Data Processing\")\n",
    "print(\"=\" * 60)\n",
    "generate_code(\n",
    "    \"Read CSV file and filter rows where age > 25\",\n",
    "    code_gen_model,\n",
    "    max_length=150,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE 3: Web Request Handler\")\n",
    "print(\"=\" * 60)\n",
    "generate_code(\n",
    "    \"Function to make HTTP GET request and handle errors\",\n",
    "    code_gen_model,\n",
    "    max_length=150,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46bbf5",
   "metadata": {},
   "source": [
    "## 6. Advanced Features: Summarization & Q&A\n",
    "\n",
    "Bonus capabilities for text understanding and content creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f765396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summarization model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Summarization Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    summarization_model = load_model_with_status(\"facebook/bart-large-cnn\", \"summarization\")\n",
    "    summarization_loaded = True\n",
    "except:\n",
    "    try:\n",
    "        summarization_model = load_model_with_status(\"sshleifer/distilbart-cnn-6-6\", \"summarization\")\n",
    "        summarization_loaded = True\n",
    "    except:\n",
    "        summarization_loaded = False\n",
    "        print(\"‚ö† Summarization model not available\")\n",
    "\n",
    "def summarize_text(text, max_length=100, min_length=50):\n",
    "    \"\"\"\n",
    "    Summarize a long text into a shorter version\n",
    "    \n",
    "    Parameters:\n",
    "    - text: Text to summarize\n",
    "    - max_length: Maximum length of summary\n",
    "    - min_length: Minimum length of summary\n",
    "    \"\"\"\n",
    "    if not summarization_loaded:\n",
    "        print(\"‚ö† Summarization model not loaded\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüìù Summarizing text...\")\n",
    "    print(f\"Original length: {len(text.split())} words\")\n",
    "    \n",
    "    try:\n",
    "        summary = summarization_model(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        summary_text = summary[0]['summary_text']\n",
    "        print(f\"Summary length: {len(summary_text.split())} words\")\n",
    "        print(f\"\\n--- Summary ---\")\n",
    "        print(summary_text)\n",
    "        \n",
    "        return summary_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example text for summarization\n",
    "sample_text = \"\"\"\n",
    "Artificial Intelligence has rapidly transformed from a niche research field to \n",
    "one of the most influential technologies of our time. Machine learning models, \n",
    "particularly deep neural networks, have achieved remarkable breakthroughs in \n",
    "various domains including computer vision, natural language processing, and \n",
    "game playing. Recent advances in transformer architectures have led to the \n",
    "development of large language models that can understand and generate human-like \n",
    "text at scale. These models are now being integrated into countless applications, \n",
    "from virtual assistants to content creation tools. However, alongside these \n",
    "impressive capabilities, important questions about AI safety, bias, and ethics \n",
    "have emerged. Researchers and organizations worldwide are working to ensure that \n",
    "AI systems are developed responsibly and aligned with human values. The future \n",
    "of AI promises even greater capabilities, but also requires careful consideration \n",
    "of potential risks and societal impacts.\n",
    "\"\"\"\n",
    "\n",
    "if summarization_loaded:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE: Text Summarization\")\n",
    "    print(\"=\" * 60)\n",
    "    summarize_text(sample_text, max_length=60, min_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Question Answering model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Question Answering Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    qa_model = load_model_with_status(\"deepset/distilbert-base-uncased-squad\", \"question_answering\")\n",
    "    qa_loaded = True\n",
    "except:\n",
    "    qa_loaded = False\n",
    "    print(\"‚ö† Question Answering model not available\")\n",
    "\n",
    "def answer_question(context, question):\n",
    "    \"\"\"\n",
    "    Answer questions based on provided context\n",
    "    \n",
    "    Parameters:\n",
    "    - context: Reference text\n",
    "    - question: Question to answer\n",
    "    \"\"\"\n",
    "    if not qa_loaded:\n",
    "        print(\"‚ö† QA model not loaded\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    \n",
    "    try:\n",
    "        result = qa_model(question=question, context=context)\n",
    "        \n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"Confidence: {result['score']:.2%}\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example context and questions\n",
    "qa_context = \"\"\"\n",
    "Python is a high-level, interpreted programming language known for its simplicity \n",
    "and readability. Created by Guido van Rossum and first released in 1991, Python has \n",
    "become one of the most popular programming languages in the world. It supports multiple \n",
    "programming paradigms including procedural, object-oriented, and functional programming. \n",
    "Python is widely used in web development, data analysis, artificial intelligence, \n",
    "scientific computing, and automation. The language emphasizes code readability with \n",
    "its use of significant indentation and clean syntax. The Python Package Index (PyPI) \n",
    "provides access to hundreds of thousands of libraries that extend Python's functionality.\n",
    "\"\"\"\n",
    "\n",
    "if qa_loaded:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMPLE: Question Answering\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    questions = [\n",
    "        \"When was Python created?\",\n",
    "        \"What programming paradigms does Python support?\",\n",
    "        \"What is the Python Package Index?\"\n",
    "    ]\n",
    "    \n",
    "    for q in questions:\n",
    "        answer_question(qa_context, q)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00022db2",
   "metadata": {},
   "source": [
    "## 7. Comparison & Performance Analysis\n",
    "\n",
    "Evaluate and compare different models on the same tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Performance comparison function\n",
    "def benchmark_text_generation(prompt, models_dict, max_length=80):\n",
    "    \"\"\"\n",
    "    Benchmark different text generation models on the same prompt\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt: Input prompt\n",
    "    - models_dict: Dictionary of model names and loaded models\n",
    "    - max_length: Generation length\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nüìä Benchmarking {len(models_dict)} models...\")\n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        if model is None:\n",
    "            print(f\"‚äò {model_name}: Not loaded\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            output = model(\n",
    "                prompt,\n",
    "                max_length=max_length,\n",
    "                temperature=0.7,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            generated = output[0]['generated_text']\n",
    "            \n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Time (s)': f\"{elapsed:.3f}\",\n",
    "                'Output Length': len(generated.split()),\n",
    "                'Status': '‚úì'\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì {model_name}\")\n",
    "            print(f\"  Time: {elapsed:.3f}s | Output length: {len(generated.split())} words\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Time (s)': 'Error',\n",
    "                'Output Length': 'N/A',\n",
    "                'Status': '‚úó'\n",
    "            })\n",
    "            print(f\"‚úó {model_name}: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Display results as table\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"BENCHMARK RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(df.to_string(index=False))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "models_to_compare = {\n",
    "    \"DistilGPT-2\": gpt2_model,\n",
    "    \"CodeGen\": code_gen_model if code_gen_loaded else None,\n",
    "}\n",
    "\n",
    "benchmark_results = benchmark_text_generation(\n",
    "    \"The key to success in machine learning is\",\n",
    "    models_to_compare,\n",
    "    max_length=80\n",
    ")\n",
    "\n",
    "# System information summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {device.upper()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Available: Yes\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(f\"CUDA Available: No (running on CPU)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì Different models have different strengths:\")\n",
    "print(\"  - Smaller models (DistilGPT-2): Fast, low memory\")\n",
    "print(\"  - Larger models (GPT-2, CodeGen): Higher quality, more resources\")\n",
    "print(\"  - Specialized models: Better for specific tasks\")\n",
    "print(\"\\n‚úì Parameters affect output:\")\n",
    "print(\"  - Temperature: Controls creativity (0=deterministic, 1=random)\")\n",
    "print(\"  - Top-p: Nucleus sampling for diverse outputs\")\n",
    "print(\"  - max_length: Balance between quality and speed\")\n",
    "print(\"\\n‚úì Tips for production use:\")\n",
    "print(\"  - Use quantization for smaller model sizes\")\n",
    "print(\"  - Implement caching to avoid reloading models\")\n",
    "print(\"  - Consider API services for larger models\")\n",
    "print(\"  - Monitor GPU memory to avoid out-of-memory errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a748a",
   "metadata": {},
   "source": [
    "## 8. Next Steps & Resources\n",
    "\n",
    "Expand your generative AI skills and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb30a0",
   "metadata": {},
   "source": [
    "### Recommended Enhancements:\n",
    "\n",
    "1. **Hugging Face Hub Integration**\n",
    "   - Explore thousands of pre-trained models at huggingface.co\n",
    "   - Fine-tune models on your custom data\n",
    "   - Use `huggingface_hub` library for model management\n",
    "\n",
    "2. **Ollama for Local LLMs**\n",
    "   - Run large language models locally without internet\n",
    "   - Models: Llama 2, Mistral, Neural Chat, etc.\n",
    "   - Visit ollama.ai to get started\n",
    "\n",
    "3. **API Integration**\n",
    "   - OpenAI API for GPT-4 and ChatGPT\n",
    "   - Google Gemini API\n",
    "   - Anthropic Claude API\n",
    "\n",
    "4. **Advanced Techniques**\n",
    "   - Prompt engineering and optimization\n",
    "   - Few-shot learning\n",
    "   - Retrieval-Augmented Generation (RAG)\n",
    "   - Model quantization for efficiency\n",
    "   - LoRA fine-tuning for domain adaptation\n",
    "\n",
    "5. **Deployment Options**\n",
    "   - Streamlit for web interfaces\n",
    "   - FastAPI for REST APIs\n",
    "   - Gradio for quick demos\n",
    "   - Docker containerization\n",
    "   - Cloud platforms (AWS, Google Cloud, Azure)\n",
    "\n",
    "### Useful Libraries:\n",
    "```python\n",
    "# Text generation and NLP\n",
    "pip install transformers torch diffusers accelerate\n",
    "\n",
    "# API clients\n",
    "pip install openai anthropic google-generativeai\n",
    "\n",
    "# Web frameworks\n",
    "pip install streamlit fastapi gradio\n",
    "\n",
    "# Model optimization\n",
    "pip install optimum bitsandbytes\n",
    "\n",
    "# Data science\n",
    "pip install pandas scikit-learn matplotlib seaborn\n",
    "```\n",
    "\n",
    "### Learning Resources:\n",
    "- **Hugging Face Course**: huggingface.co/course\n",
    "- **Papers with Code**: paperswithcode.com\n",
    "- **Model Cards**: huggingface.co/models\n",
    "- **Prompt Engineering Guide**: github.com/dair-ai/Prompt-Engineering-Guide"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
